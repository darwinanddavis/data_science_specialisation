---
title: Forecasting exercise performance from wearable tech data using machine learning and cross validation      
author: Matthew Malishev     
fontsize: 10
geometry: margin=1in
documentclass: article
linkcolor: blue
urlcolor: blue
citecolor: red
output:
  html_document:
    highlight: tango
    code_folding: show
    code_download: true
    toc: yes
    toc_depth: 4
    number_sections: no
    toc_float: yes
  pdf_document:
    includes:
      in_header: # add .tex file with header content
    highlight: tango
    template: null
    toc: yes
    toc_depth: 4
    number_sections: false
    fig_width: 4
    fig_height: 5
    fig_caption: true
    df_print: tibble 
    citation_package: biblatex # natbib
    latex_engine: xelatex #pdflatex # lualatex
    keep_tex: true # keep .tex file in dir 
  word_document:
    highlight: tango
    keep_md: yes
    pandoc_args: --smart
    #reference: mystyles.docx
    toc: yes
inludes:
  before_body: before_body.tex
subtitle: 
tags:
- nothing
- nothingness
params: 
  date: !r Sys.Date()
  session: !r sessionInfo()  
  version: !r getRversion()
classoption: portrait
---

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
</script>

```{r, set-options, echo = FALSE, cache = FALSE}
options(width=100)
knitr::opts_chunk$set(
 eval = T, # run all code
 echo = T, # show code chunks in output
 comment = "",
 tidy.opts=list(width.cutoff=100), # set width of code chunks in output
 tidy=T, # make output as tidy
 message = F,  # mask all messages
 warning = F, # mask all warnings 
 size="small" # set code chunk size
)

# https://github.com/ucb-stat133/stat133-fall-2016/blob/master/hws/hw02-tables-ggplot.Rmd
knitr::opts_knit$set(root.dir=paste0(params$dir,"/")) # set working dir


```

\newpage  

### The compiled HTML report is found at this link: https://darwinanddavis.github.io/data_science_specialisation/ml/ass/malishev_ml_ass.html  

\newpage   

Date: `r params$date`  
R version: `r params$version`  

\  

R session info 

```{r, echo=T}
params$session
```      

\newpage  

## Overview

This project uses learning models to predict how well participants with wearable fit tech performed exercises. The aim is to use these performance data to improve data quality for both user and future applications of wearable tech.   
  
Data readings were taken from 10 reps for five different movements, one correct (A) and four incorrect (B-D):  
\  
* A: according to instructions (Class A)  
* B: throwing the elbows to the front (Class B)  
* C: lifting the weight halfway (Class C)  
* D: lowering the weight halfway (Class D)  
* E: throwing the hips to the front (Class E)  
  
A series of learning models were applied to test how well the training data could predict the four incorrect exercise movements in the test data.        

```{r, load packages, include=T, cache=F, message=F, warning=F, results='hide'}
require(caret)
require(dplyr)
require(readr)
require(here)
require(randomForest)
require(rattle)
```

Read in the data  
```{r,echo=F}
# load data  
complete_data <- "https://github.com/darwinanddavis/data_science_specialisation/raw/gh-pages/ml/ass/pml-complete.csv" %>% read_csv
training_data <-  "https://github.com/darwinanddavis/data_science_specialisation/raw/gh-pages/ml/ass/pml-training.csv" %>% read_csv
testing_data <-  "https://github.com/darwinanddavis/data_science_specialisation/raw/gh-pages/ml/ass/pml-testing.csv" %>% read_csv
```

Visualise the data  
```{r}
training_data %>% names
```


After reading in the data, remove all the variables with zero variance     
```{r}
nzvar <- nearZeroVar(training_data)
training_data <- training_data[,-nzvar]
testing_data <- testing_data[,-nzvar]
dim(training_data)
dim(testing_data)
```


### Split the training and testing data 

Set the seed and remove NAs from the data that will confound the predictions. Set the threshold to 90% to make it interesting.  
```{r}
set.seed(12)
# remove nas
rmna <- sapply(training_data, function(x) mean(is.na(x))) > 0.9
training_data <- training_data[,rmna == F]
testing_data <- testing_data[,rmna == F]

```

We also need to remove the unneeded variables that won't be part of the predictions, then we can split the data into training and testing sets.  
```{r}
training_data <- training_data[,-c(1:7)] # remove unneeded variables 
testing_data <- testing_data[,-c(1:7)] 

inTrain = createDataPartition(y=training_data$classe,
                              p=0.5, 
                              list = F)
training = training_data[inTrain,]
testing = training_data[-inTrain,]

colnames(training) <- make.names(colnames(training))
colnames(testing) <- make.names(colnames(testing))
training$classe <- factor(as.character(training$classe))
testing$classe <- factor(as.character(testing$classe))
```


## Model 1: Classification tree  

First, the training model was built from the training dataset for each of the exercise variables of interest for a user.   
  
The idea then is to classify the data by building a classification tree. We use the training data to train the model for the classification, then test it against the testing data for the variables of interest.    

```{r}
mod1 <- train(classe ~ .,
                method="rpart",
                data=training)
mod1fin <- mod1$finalModel
```


We can see how much variation the classification captures by summarising the final model prediction on the training data, then plotting the classification tree.  
```{r}
mod1fin
```


```{r}
require(rattle)
fancyRpartPlot(mod1fin)
```

Then we predict the new values for the testing data using the trained classification model and get the accuracy.  
```{r}
# predict against testing data 
pred1 <- predict(mod1, testing)
# get accuracy 
sum(pred1 == testing$classe) / length(pred1) 
```

## Model 2: Bagging       

We can test another model to see if it performs better, such as a boottrapping with aggregation, i.e. bagging.  

The model works by averaging complicated models together to get a smoother model fit that gives a balance between potential bias and variance in model fit.  

The training data metrics were then tested against the testing data to determine how well a boosting model performed in predicting the outcomes.  

```{r}
mod2 <- train(classe ~ .,
              method="treebag",
              coob=T,
              keepX=T,
              data=training)
mod2fin <- mod2$finalModel
# prediction for bagging method 
pred2 <- predict(mod2,testing)
sum(pred2 == testing$classe) / length(pred2) 
```

## Model 3: Boosting (GBM)    

Finally, we test another boosting method for the predictors (GBM model).    
```{r}
require(randomForest)
mod3 <- train(classe ~ .,
              method = "gbm", 
              verbose = F,
              data = training)
mod3fin <- mod3$finalModel
pred3 <- predict(mod3,testing)
confusionMatrix(pred3, testing$classe)
sum(pred3 == testing$classe) / length(pred3) 
```


<!-- Finally, we pull the overall statistics from the model fit to verify it's the better option.   -->

<!-- ```{r} -->
<!-- tb_code <- getModelInfo("treebag")[[1]] -->
<!-- tb_code$oob(mod2fin) # get out of bag error and stats   -->
<!-- ``` -->
  
## Combined model  
  
By combining the model predictions, we can cross validate how well they perform on the testing data using a GAM method.        
```{r}
pred_df <- data.frame("Classification tree" = pred1,
                    "Bagging" = pred2,
                    "GBM" = pred3,
                    classe=testing$classe)
modcom <- train(classe ~.,
                method="gam", # apply a gam method for predicting all models  
                data=pred_df)
modcomfin <- modcom$finalModel # get final final 
modcom_pred <- predict(modcom,pred_df) # test against testing data  

# run diagnostics  
table(modcom_pred,testing$classe) 
confusionMatrix(table(modcom_pred,testing$classe))
```

## Conclusion  

The bagging model outperforms both other methods due to it's higher accuracy, so we choose this model as the final model for the testing dataset.    

```{r}
data.frame(
  "model" = c("Classification tree","Bagging","GBM","GAM"),
  "accuracy" = c(
    sum(pred1 == testing$classe) / length(pred1),
    sum(pred2 == testing$classe) / length(pred2),
    sum(pred3 == testing$classe) / length(pred3),
    sum(modcom_pred == testing$classe) / length(modcom_pred) 
  )
)
```

With our selected model, we can test how well it performs on the testing data set.

```{r}
pred_final <- predict(mod2,testing_data)
pred_final[1:20] 
```



